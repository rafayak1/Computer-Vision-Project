{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCs4Smmjiw-0",
        "outputId": "0a0bc507-c784-4644-fd4c-3b62ddcc3509"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "# import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOOxwOq8i3HO",
        "outputId": "f6c4e1a2-6898-44c4-ed2f-ad22fb6605a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OpenCV: Couldn't read video stream from file \"video.hevc\"\n",
            "[ERROR:0@74.428] global cap.cpp:166 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
            "\n",
            "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): video.hevc in function 'icvExtractPattern'\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "video_path= 'video.hevc'\n",
        "frames = list()\n",
        "\n",
        "# Load the video\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "while True:\n",
        "  # Read the next frame\n",
        "  ret, frame = video.read()\n",
        "  if not ret:\n",
        "    break\n",
        "\n",
        "  frame = cv2.resize(frame, (512, 512))\n",
        "  # Convert image in BGR format to RGB.\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  # Add a batch dimension which is required by the model.\n",
        "  frame = np.expand_dims(frame, axis=0)/255.0\n",
        "\n",
        "  frames.append(frame)\n",
        "  if len(frames) == 800:\n",
        "    break\n",
        "\n",
        "frames = np.array(frames)\n",
        "frames.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoD5jM9qi93R",
        "outputId": "d65d7849-1292-445b-df47-970d2baefb09"
      },
      "outputs": [],
      "source": [
        "model_url =  'https://tfhub.dev/google/HRNet/camvid-hrnetv2-w48/1'\n",
        "seg_model = hub.load(model_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KCXK_zZ0SUnX"
      },
      "outputs": [],
      "source": [
        "# Function to convert a single channel mask representation to an RGB mask.\n",
        "def class_to_rgb(mask_class):\n",
        "\n",
        "    # Create RGB channels\n",
        "    r_map = np.zeros_like(mask_class).astype(np.uint8)\n",
        "    g_map = np.zeros_like(mask_class).astype(np.uint8)\n",
        "    b_map = np.zeros_like(mask_class).astype(np.uint8)\n",
        "\n",
        "\n",
        "    class_id=17\n",
        "    index = mask_class == class_id\n",
        "\n",
        "    if class_id == 17:\n",
        "      r_map[index] = 128\n",
        "      g_map[index] = 64\n",
        "      b_map[index] = 128\n",
        "\n",
        "    seg_map_rgb = np.stack([r_map, g_map, b_map], axis=2)\n",
        "\n",
        "    return seg_map_rgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EWoZaGgNTJ4T"
      },
      "outputs": [],
      "source": [
        "\n",
        "def image_overlay(image, seg_map_rgb):\n",
        "\n",
        "\n",
        "    image = (image*255.0).astype(np.uint8)\n",
        "\n",
        "\n",
        "    image = cv2.addWeighted(image, 1.0, cv2.cvtColor(cv2.cvtColor(seg_map_rgb, cv2.COLOR_RGB2BGR), cv2.COLOR_RGB2BGR), 0.6, 0.0)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z8HumKFjTPEO"
      },
      "outputs": [],
      "source": [
        "def segment(images, model):\n",
        "    overlayed_images = list()\n",
        "\n",
        "    for img in images:\n",
        "\n",
        "        # Forward pass through the model (convert the tensor output to a numpy array).\n",
        "        pred_mask = model.predict(img).numpy()\n",
        "\n",
        "        # Remove the background class added by the model.\n",
        "        pred_mask = pred_mask[:,:,:,1:]\n",
        "\n",
        "        # Remove the batch dimension.\n",
        "        pred_mask = np.squeeze(pred_mask)\n",
        "        pred_mask_class = np.argmax(pred_mask, axis=-1)\n",
        "\n",
        "        # Convert the predicted (class) segmentation map to a color segmentation map.\n",
        "        pred_mask_rgb = class_to_rgb(pred_mask_class)\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Display the original image.\n",
        "        ax1 = fig.add_subplot(1,3,1)\n",
        "        ax1.imshow(img[0])\n",
        "        ax1.title.set_text('Input Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display the predicted color segmentation mask.\n",
        "        ax2 = fig.add_subplot(1,3,2)\n",
        "        ax2.set_title('Predicted Mask')\n",
        "        ax2.imshow(pred_mask_rgb)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display the predicted color segmentation mask overlayed on the original image.\n",
        "        overlayed_image = image_overlay(img[0], pred_mask_rgb)\n",
        "        ax4 = fig.add_subplot(1,3,3)\n",
        "        ax4.set_title('Overlayed Image')\n",
        "        ax4.imshow(overlayed_image)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "        overlayed_images.append(overlayed_image)\n",
        "    return overlayed_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QZ9Dw0H0TXwK",
        "outputId": "b7bd8d71-839f-455e-8f2f-a1b34c4c70bc"
      },
      "outputs": [],
      "source": [
        "images = segment(frames, seg_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLNTmLeThIr6"
      },
      "outputs": [],
      "source": [
        "# Specify the desired video format and frame rate\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "fps = 20\n",
        "\n",
        "# Create a VideoWriter object\n",
        "video_writer = cv2.VideoWriter('output.avi', fourcc, fps, (images[0].shape[1], images[0].shape[0]))\n",
        "\n",
        "# Iterate through the list of images\n",
        "for image in images:\n",
        "    # Write the frame to the video\n",
        "    video_writer.write(image)\n",
        "\n",
        "# Close the VideoWriter object\n",
        "video_writer.release()\n",
        "\n",
        "print('Video saved as output.avi')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
